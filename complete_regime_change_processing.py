#!/usr/bin/env python3
"""
Complete the processing of the existing regime change book
It's in the database but has 0 chunks - need to chunk and embed it
"""

import asyncio
import sys
import time
import os
from pathlib import Path
from datetime import datetime

# Set embedding dimension to match our existing data
os.environ['EMBEDDING_DIMENSION'] = '384'

# Add src to path
sys.path.append('src')

from src.core.sqlite_storage import SQLiteStorage
from src.core.qdrant_storage import QdrantStorage
from src.ingestion.pdf_parser import PDFParser
from src.ingestion.text_chunker import TextChunker, ChunkingConfig
from src.ingestion.local_embeddings import LocalEmbeddingGenerator

async def complete_book_processing(book_id: str):
    """
    Complete processing for a book that exists but has no chunks
    """
    
    print(f"ğŸ”„ Completing processing for book: {book_id}")
    print()
    
    # Initialize components
    print("ğŸ”§ Initializing components...")
    storage = SQLiteStorage()
    qdrant_storage = QdrantStorage()
    
    # Get the existing book
    print("ğŸ“– Retrieving book from database...")
    book = await storage.get_book(book_id)
    if not book:
        print(f"âŒ Book not found: {book_id}")
        return False
    
    print(f"âœ… Found book: {book.title}")
    print(f"   ğŸ‘¤ Author: {book.author}")
    print(f"   ğŸ“„ Pages: {book.total_pages}")
    print(f"   ğŸ“¦ Current chunks: {book.total_chunks}")
    print(f"   ğŸ“ File: {book.file_path}")
    print()
    
    # Check if file still exists
    if not Path(book.file_path).exists():
        print(f"âŒ File not found: {book.file_path}")
        return False
    
    # Initialize processing components
    pdf_parser = PDFParser()
    chunker = TextChunker(ChunkingConfig(
        chunk_size=1000,
        chunk_overlap=200,
        min_chunk_size=100,
        max_chunk_size=2000
    ))
    
    # Parse PDF again (we need the text content)
    print("ğŸ“– Re-parsing PDF for text content...")
    start_time = time.time()
    
    try:
        parse_result = pdf_parser.parse_file(Path(book.file_path))
    except Exception as e:
        print(f"âŒ PDF parsing failed: {e}")
        print("ğŸ’¡ This might be a scanned PDF requiring OCR")
        return False
    
    parse_time = time.time() - start_time
    
    if parse_result['errors']:
        print(f"âš ï¸  Parse warnings: {parse_result['errors']}")
    
    print(f"âœ… PDF re-parsed in {parse_time:.2f}s")
    print(f"ğŸ“„ Extracted {len(parse_result['pages'])} pages of text")
    print()
    
    # Check if we got any text
    total_text = sum(len(page.get('text', '')) for page in parse_result['pages'])
    if total_text < 1000:
        print(f"âš ï¸  Very little text extracted ({total_text} characters)")
        print("ğŸ’¡ This might be a scanned PDF that needs OCR processing")
    
    # Create chunks
    print("âœ‚ï¸  Creating chunks...")
    start_time = time.time()
    chunks = chunker.chunk_pages(parse_result['pages'], book.id, {})
    chunk_time = time.time() - start_time
    
    print(f"âœ… Created {len(chunks)} chunks in {chunk_time:.2f}s")
    
    if len(chunks) == 0:
        print("âŒ No chunks were created - likely a scanned PDF")
        print("ğŸ’¡ Consider using OCR tools to extract text first")
        return False
    
    print()
    
    # Save chunks to SQLite
    print("ğŸ’¾ Saving chunks to SQLite...")
    success = await storage.save_chunks(chunks)
    
    if not success:
        print("âŒ Failed to save chunks to SQLite")
        return False
    
    # Update book with chunk count
    book.total_chunks = len(chunks)
    book.indexed_at = datetime.now()
    await storage.update_book(book)
    
    print(f"âœ… {len(chunks)} chunks saved to SQLite")
    print()
    
    # Generate embeddings and store in Qdrant
    print("ğŸ§  Generating embeddings for Qdrant...")
    start_time = time.time()
    
    embedding_generator = LocalEmbeddingGenerator()
    
    try:
        # Fix model name issue
        embedding_generator.model_name = "nomic-embed-text:latest"
        embedding_generator.embedding_dimension = 384
        
        async with embedding_generator as gen:
            # Process in smaller batches for stability
            batch_size = 8
            total_added = 0
            
            print(f"ğŸ”„ Processing {len(chunks)} chunks in batches of {batch_size}")
            
            for i in range(0, len(chunks), batch_size):
                batch_chunks = chunks[i:i + batch_size]
                batch_num = i // batch_size + 1
                total_batches = (len(chunks) + batch_size - 1) // batch_size
                
                print(f"  ğŸ“ˆ Processing batch {batch_num}/{total_batches} ({len(batch_chunks)} chunks)")
                
                try:
                    # Generate embeddings for this batch
                    batch_embeddings = await gen.generate_embeddings(batch_chunks)
                    
                    # Prepare documents for Qdrant
                    documents = []
                    for chunk, embedding in zip(batch_chunks, batch_embeddings):
                        documents.append({
                            'id': chunk.id,
                            'vector': embedding,
                            'payload': {
                                'book_id': chunk.book_id,
                                'text': chunk.text,
                                'chunk_index': chunk.chunk_index,
                                'page_start': chunk.page_start,
                                'page_end': chunk.page_end,
                                'book_title': book.title,
                                'book_author': book.author,
                                'categories': book.categories
                            }
                        })
                    
                    # Add to Qdrant
                    success = await qdrant_storage.add_documents(documents)
                    if success:
                        total_added += len(documents)
                        print(f"    âœ… Added {len(documents)} vectors to Qdrant")
                    else:
                        print(f"    âŒ Failed to add batch {batch_num} to Qdrant")
                        
                except Exception as e:
                    print(f"    âŒ Error processing batch {batch_num}: {e}")
                    continue
    
    except Exception as e:
        print(f"âŒ Embedding generation failed: {e}")
        print("ğŸ’¡ Continuing without vector embeddings - text search still available")
        total_added = 0
    
    embedding_time = time.time() - start_time
    print(f"âœ… Embedding processing completed in {embedding_time:.2f}s")
    print(f"âš¡ Added {total_added} vectors to Qdrant")
    print()
    
    # Final summary
    print("ğŸ‰ PROCESSING COMPLETE!")
    print("=" * 60)
    print(f"ğŸ“š Book: {book.title}")
    print(f"ğŸ‘¤ Author: {book.author}")
    print(f"ğŸ“„ Pages: {book.total_pages}")
    print(f"ğŸ“¦ Chunks: {book.total_chunks}")
    print(f"ğŸ·ï¸  Categories: {', '.join(book.categories) if book.categories else 'None'}")
    print(f"âš¡ Vectors: {total_added}")
    print()
    print("ğŸ’¾ FINAL STATUS:")
    print(f"  ğŸ“ SQLite: âœ… Book and {book.total_chunks} chunks")
    if total_added > 0:
        print(f"  âš¡ Qdrant: âœ… {total_added} vectors")
        print("  ğŸ” Search: âœ… Both text and semantic search available")
    else:
        print(f"  âš¡ Qdrant: âš ï¸  No vectors (embeddings failed)")
        print("  ğŸ” Search: âš ï¸  Only text search available")
    
    return True

async def main():
    """Main function"""
    # Hard-coded book ID from our query above
    book_id = "detecting_regime_change_in_computational_finance_data_science_machine_learning_and_algorithmic_trading_c96b7af5"
    
    print("ğŸ¯ COMPLETING REGIME CHANGE BOOK PROCESSING")
    print("=" * 60)
    print()
    
    success = await complete_book_processing(book_id)
    
    if success:
        print("\nğŸš€ Book processing completed!")
        print("\nğŸ” Try these searches:")
        print('   python search_book.py "regime change"')
        print('   python search_book.py "machine learning finance"')
        print('   python search_book.py "data science trading"')
    else:
        print("\nğŸ’¥ Processing failed!")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())